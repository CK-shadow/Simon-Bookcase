{"code":"(window.webpackJsonp=window.webpackJsonp||[]).push([[160],{627:function(a,t,e){\"use strict\";e.r(t);var r=e(0),n=Object(r.a)({},(function(){var a=this,t=a.$createElement,e=a._self._c||t;return e(\"ContentSlotsDistributor\",{attrs:{\"slot-key\":a.$parent.slotKey}},[e(\"blockquote\",[e(\"p\",[a._v(\"Kafka为数据管道带来的主要价值在于，它可以作为数据管道各个数据段之间的大型缓冲区，有效地解耦管道数据的生产者和消费者。Kafka的解耦能力以及在安全和效率方面的可靠性，使它成为构建数据管道的最佳选择\")])]),a._v(\" \"),e(\"h2\",{attrs:{id:\"构建数据管道时需要考虑的问题\"}},[e(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#构建数据管道时需要考虑的问题\"}},[a._v(\"#\")]),a._v(\" 构建数据管道时需要考虑的问题\")]),a._v(\" \"),e(\"h3\",{attrs:{id:\"及时性\"}},[e(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#及时性\"}},[a._v(\"#\")]),a._v(\" 及时性\")]),a._v(\" \"),e(\"p\",[a._v(\"有些系统希望每天一次性地接收大量数据，而有些则希望在数据生成几毫秒之内就能拿到它们。大部分数据管道介于这两者之间。一个好的数据集成系统能够很好地支持数据管道的各种及时性需求，而且在业务需求发生变更时，具有不同及时性需求的数据表之间可以方便地进行迁移。Kafka作为一个基于流的数据平台，提供了可靠且可伸缩的数据存储，可以支持几近实时的数据管道和基于小时的批处理。生产者可以频繁地向Kafka写入数 据，也可以按需写入：消费者可以在数据到达的第一时间读取它们，也可以每隔一段时间读取一次积压的数据\")]),a._v(\" \"),e(\"p\",[e(\"br\"),a._v(\"\\nKafka在这里扮演了一个大型缓冲区的角色，降低了生产者和消费者之间的时间敏感度。实时的生产者和基于批处理的消费者可以同时存在，也可以任意组合。实现回压策略也因此变得更加容易，Kafka本身就使用了回压策略（必要时可以延后向生产者发送确认），消费速率完全取决于消费者自己\")]),a._v(\" \"),e(\"h3\",{attrs:{id:\"可靠性\"}},[e(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#可靠性\"}},[a._v(\"#\")]),a._v(\" 可靠性\")]),a._v(\" \"),e(\"p\",[a._v(\"数据传递保证是可靠性的另一个重要因素。有些系统允许数据丢失，不过在大多数情况下，它们要求至少一次传递。也就是说，源系统的每一个事件都必须到达目的地，不过有时候需要进行重试，而重试可能造成重复传递。有些系统甚至要求仅一次传递--源系统的每一个事件都必须到达目的地，不允许丢失，也不允许重复\")]),a._v(\" \"),e(\"p\",[e(\"br\"),a._v('\\nKafka本身就支持\"至少一次传递\"，如果再结合具有事务模型或唯一键特性的外部存储系统，Kafka也能实现\"仅一次传递\"。因为大部分的端点都是数据存储系统，它们提供了\"仅一次传递\"的原语支持，所以基于Kafka的数据管道也能实现\"仅一次传递\"。值得一捷的是，Connect API为集成外部系统提供了处理偏移量的API，连接器因此可以构建仅一次传递的端到端数据管道。实际上，很多开源的连接器都支持仅一次传递')]),a._v(\" \"),e(\"h3\",{attrs:{id:\"高吞吐量和动态吞吐量\"}},[e(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#高吞吐量和动态吞吐量\"}},[a._v(\"#\")]),a._v(\" 高吞吐量和动态吞吐量\")]),a._v(\" \"),e(\"p\",[a._v(\"为了满足现代数据系统的要求，数据管道需要支持非常高的吞吐量。更重要的是，在某些情况下，数据管道还需要能够应对突发的吞吐量增长。由于我们将Kafka作为生产者和消费者之间的缓冲区，消费者的吞吐量和生产者的吞吐量就不会耦合在一起了。我们也不再需要实现复杂的回压机制，如果生产者的吞吐量超过了消费者的吞吐量，可以把数据积压在Kafka里，等待消费者追赶上来。通过增加额外的消费者或生产者可以实现Kafka的伸缩，因此我们可以在数据管道的任何一边进行动态的伸缩，以便满足持续变化的需求\")]),a._v(\" \"),e(\"p\",[e(\"br\"),a._v(\"\\n因为Kafka是一个高吞吐量的分布式系统，一个适当规模的集群每秒钟可以处理数百兆的数据，所以根本无需担心数据管道无法满足伸缩性需求。另外，Connect API不仅支持伸缩，而且擅长并行处理任务。Kafka支持多种类型的压缩，在增长吞吐量时，Kafka用户和管理员可以通过压缩来调整网络和存储资源的使用\")]),a._v(\" \"),e(\"h3\",{attrs:{id:\"数据格式\"}},[e(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#数据格式\"}},[a._v(\"#\")]),a._v(\" 数据格式\")]),a._v(\" \"),e(\"p\",[a._v(\"Kafka和Connect API与数据格式无关。生产者和消费者可以使用各种序列化器来表示任意格式的数据。Connect API有自己的内存对象模型，包括 数据类型和schema。不过，可以使用一些可插拔的转换器将这些对象保存成任意的格式，也就是说，不管数据是什么格式的，都不会限制我们使用连接器\")]),a._v(\" \"),e(\"h3\",{attrs:{id:\"转换\"}},[e(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#转换\"}},[a._v(\"#\")]),a._v(\" 转换\")]),a._v(\" \"),e(\"p\",[a._v(\"数据转换比其他需求更具争议性。数据管道的构建可以分为两大阵营，即ETL和ELT。ETL表示提取一转换一加载（Extract-Transform-Load），也就是说，当数据流经数据管道时，数据管道会负责处理它们。这种方式为我们节省了时间和存储空间，因为不需要经过保存数据、修改数据、再保存数据这样的过程。不过，这种好处也要视情况而定。有时候，这种方式会给我们带来实实在在的好处，但也有可能给数据管道造成不适当的计算和 存储负担。这种方式有一个明显不足，就是数据的转换会给数据管道下游的应用造成一些限制，特别是当下游的应用希望对数据进行进一步处理的时候\")]),a._v(\" \"),e(\"p\",[e(\"br\"),a._v('\\nELT表示提取－加载－转换（Extract-Load-Transform）。在这种模式下，数据管道只做少量的转换（主要是数据类型转换），确保到达数据地的数据尽可能地与数据源保持一致。这种情况也被称为高保真（high fidelity）数据管道或数据湖（data lake）架构。目标系统收集 \"原始数据\"，并负责处理它们。这种方式为目标系统的用户提供了最大的灵活性，因为它们可以访问到完整的数据。在这些系统里诊断问题也变得更加容易，因为数据被集中在同一个系统里进行处理，而不是分散在数据管道和其他应用里。这种方式的不足在于，数据的转换占用了目标系统太多的CPU和存储资源。有时候，目标系统造价高昂，如果有可能，人们希望能够将计算任务移出这些系统')]),a._v(\" \"),e(\"h3\",{attrs:{id:\"安全性\"}},[e(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#安全性\"}},[a._v(\"#\")]),a._v(\" 安全性\")]),a._v(\" \"),e(\"p\",[a._v(\"安全性是人们一直关心的问题。对于数据管道的安全性来说，人们主要关心如下几个方面：\")]),a._v(\" \"),e(\"ul\",[e(\"li\",[a._v(\"我们能否保证流经数据管道的数据是经过加密的？这是跨数据中心数据管道通常需要考虑的一个主要方面\")]),a._v(\" \"),e(\"li\",[a._v(\"谁能够修改数据管道？\")]),a._v(\" \"),e(\"li\",[a._v(\"如果数据管道需要从一个不受信任的位置读取或写入数据，是否有适当的认证机制\")])]),a._v(\" \"),e(\"p\",[a._v(\"Kafka支持加密传输数据，从数据源到Kafka，再从Kafka到数据池。它还支持认证（通过SASL来实现）和授权，所以你可以确信，如果一个主题\\n包含了敏感信息，在不经授权的情况下，数据是不会流到不安全的系统里的。Kafka还提供了审计日志用于跟踪访问记录。通过编写额外的代码，可能跟踪到每个事件的来源和事件的修改者，从而在每个记录之间建立起整体的联系\")]),a._v(\" \"),e(\"h3\",{attrs:{id:\"故障处理能力\"}},[e(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#故障处理能力\"}},[a._v(\"#\")]),a._v(\" 故障处理能力\")]),a._v(\" \"),e(\"p\",[a._v(\"Kafka会长时间地保留数据，所以我们可以在适当的时候回过头来重新处理出错的数据\")]),a._v(\" \"),e(\"h2\",{attrs:{id:\"如何在connect-api和客户端api之间做出选择\"}},[e(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#如何在connect-api和客户端api之间做出选择\"}},[a._v(\"#\")]),a._v(\" 如何在Connect API和客户端API之间做出选择\")]),a._v(\" \"),e(\"p\",[a._v(\"Kafka客户端是要被内嵌到应用程序里的，应用程序使用它们向Kafka写入数据或从Kafka读取数据。如果你是开发人员，你会使用Kafka客户端将应用程序连接到Kafka，并修改应用程序的代码，将数据推送到Kafka或者从Kafka读取数据。如果要将Kafka连接到数据存储系统，可以使用 Connect，因为这些系统不是你开发的，你不能或者也不想修改它们的代码。Connect可以用于从外部数据存储系统读取数据，或者将数据推送到外部存储系统。如果数据存储系统提供了相应的连接器，那么非开发人员 就可以通过配置连接器的方式来使用 Connect\")]),a._v(\" \"),e(\"p\",[e(\"br\"),a._v(\"\\n如果你要连接的数据存储系统没有相应的连接器，那么可以考虑使用客户端API或Connect API开发一个应用程序。我们建议首选Connect，因为它提供了一些开箱即用的特性，比如配置管理、偏移量存储、并行处理、错误处理，而且支持多种数据类型和标准的REST管理 API。开发一个连接Kafka和外部数据存储系统的小应用程序看起来很简单，但其实还有很多细节需要处理，比如数据类型和配置选项，这些无疑加大了开发的复杂性一--Connect处理了大部分细节，让你可以专注于数据的传输\")]),a._v(\" \"),e(\"h2\",{attrs:{id:\"kafka-connect\"}},[e(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#kafka-connect\"}},[a._v(\"#\")]),a._v(\" Kafka Connect\")]),a._v(\" \"),e(\"p\",[a._v(\"Connect是Kafka的一部分，它为在Kafka和外部数据存储系统之间移动数据提供了一种可靠且可伸缩的方式。它为连接器插件提供了一组API和一个运行时--Connect负责运行这些插件，它们则负责移动数据。Connect以worker进程集群的方式运行，我们基于worker进程安装连接器插件，然后使用REST API来管理和配置connector，这些worker进程都是长时间持续运行的作业。连接器启动额外的task，有效地利用工作节点的资源，以并行的方式移动大量的数据。数据源的连接器负责从源系统读取数据，并把数据对象提供给worker进程。数据地的连接器负责从worker进程获取数据，并把它们写入目标系统。Connect通过connector在Kafka里存储不同格式的数据。Kafka支持JSON，而且Confluent Schema Registry提供了Avro转换器。开发人员可以选择数据的存储格式，这些完全独立于他们所使用的连接器\")]),a._v(\" \"),e(\"h3\",{attrs:{id:\"运行connect\"}},[e(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#运行connect\"}},[a._v(\"#\")]),a._v(\" 运行Connect\")]),a._v(\" \"),e(\"p\",[a._v(\"Connect随着Kafka一起发布，所以无需单独安装。如果你打算在生产环境使用Connect来移动大量的数据，或者打算运行多个连接器，那么最好把Connect部署在独立于broker的服务器上。在所有的机器上安装Kafka，并在部分服务器上启动broker，然后在其他服务器启动Connect\")]),a._v(\" \"),e(\"p\",[e(\"br\"),a._v(\"\\n启动Connect进程与启动broker差不多，在调用脚本时传入一个属性文件即可\")]),a._v(\" \"),e(\"div\",{staticClass:\"language- line-numbers-mode\"},[e(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[e(\"code\",[a._v(\"bin/connect-distributed.sh config/connect-distributed.properties\\n\")])]),a._v(\" \"),e(\"div\",{staticClass:\"line-numbers-wrapper\"},[e(\"span\",{staticClass:\"line-number\"},[a._v(\"1\")]),e(\"br\")])]),e(\"p\",[e(\"br\"),a._v(\"\\nConnect进程有以下几个重要的配置参数：\")]),a._v(\" \"),e(\"ul\",[e(\"li\",[a._v(\"bootstrap.server：该参数列出了将要与Connect协同工作的broker服务器，连接器将会向这些broker写入数据或者从它们那里读取数据。你不需要指定集群所有的broker, 不过建议至少指定3个\")]),a._v(\" \"),e(\"li\",[a._v(\"group.id：具有相同group id的worker属于同一个Connect集群。集群的连接器和它们的任务可以运行在任意一个worker上\")]),a._v(\" \"),e(\"li\",[a._v(\"key.converter和value.converter：Connect可以处理存储在Kafka里的不同格式的数据。这两个参数分别指定了消息的键和值所使用的转换器。默认使用Kafka提供的JSONConverter，当然也可以配置成Confluent Schema Registry提供的AvroConverter\")])]),a._v(\" \"),e(\"p\",[a._v(\"我们一般通过Connect REST API来配置和监控rest.host.name和rest.port连接器可以为REST API指定特定的端口\")]),a._v(\" \"),e(\"p\",[e(\"br\"),a._v(\"\\n在启动worker集群之后，可以通过REST API来验证它们是否运行正常\")]),a._v(\" \"),e(\"div\",{staticClass:\"language- line-numbers-mode\"},[e(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[e(\"code\",[a._v(\"http://localhost:8083/\\n\")])]),a._v(\" \"),e(\"div\",{staticClass:\"line-numbers-wrapper\"},[e(\"span\",{staticClass:\"line-number\"},[a._v(\"1\")]),e(\"br\")])]),e(\"p\",[a._v(\"这个REST URI应该要返回当前Connect的版本号。我们还可以检查已经安装好的连接器插件：\")]),a._v(\" \"),e(\"div\",{staticClass:\"language- line-numbers-mode\"},[e(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[e(\"code\",[a._v(\"http://localhost:8083/connector-plugins\\n\")])]),a._v(\" \"),e(\"div\",{staticClass:\"line-numbers-wrapper\"},[e(\"span\",{staticClass:\"line-number\"},[a._v(\"1\")]),e(\"br\")])])])}),[],!1,null,null,null);t.default=n.exports}}]);","extractedComments":[]}